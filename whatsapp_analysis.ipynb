{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#%% \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import *\n",
    "import datetime as dt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import regex\n",
    "import emoji\n",
    "# from seaborn import *\n",
    "# from seaborn import heatmap\n",
    "from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator\n",
    "from nltk import *\n",
    "from plotly import express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithDateAndTime(s):\n",
    "    pattern = '^([0-9]+)(/)([0-9]+)(/)([0-9]+), ([0-9]+):([0-9]+)( am| pm) -'\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# # uncomment for 24-hr format \n",
    "# def startsWithDateAndTime(s):\n",
    "#     pattern = '^([0-9]+)/([0-9]+)/([0-9]+), ([0-9]+):([0-9]+) -'\n",
    "#     result = re.match(pattern, s)\n",
    "#     if result:\n",
    "#         return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FindAuthor(s):\n",
    "    patterns = [\n",
    "        '([a-zA-Z]+):',                        # First Name\n",
    "        '([a-zA-Z]+\\s[a-zA-Z]+):',              # First Name + Last Name\n",
    "        '([a-zA-Z]+\\s[a-zA-Z]+\\s[a-zA-Z]+):',    # First Name + Middle Name + Last Name\n",
    "        '([+][0-9]{2}\\s[0-9]{5}\\s[0-9]{5}):',         # Mobile Number (India no.)\n",
    "        '([+][0-9]{2}\\s[0-9]{3}\\s[0-9]{3}\\s[0-9]{4}):',   # Mobile Number (US no.)\n",
    "        '([a-zA-Z]+[\\u263a-\\U0001f999]+:)',    # Name and Emoji              \n",
    "    ]\n",
    "    pattern = '^' + '|'.join(patterns)\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoint(line):   \n",
    "    splitLine = line.split(' - ') \n",
    "    dateTime = splitLine[0]\n",
    "    date, time = dateTime.split(', ') \n",
    "    message = ' '.join(splitLine[1:])\n",
    "    if FindAuthor(message): \n",
    "        splitMessage = message.split(': ') \n",
    "        author = splitMessage[0] \n",
    "        message = ' '.join(splitMessage[1:])\n",
    "    else:\n",
    "        author = None\n",
    "    return date, time, author, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = [] # List to keep track of data so it can be used by a Pandas dataframe\n",
    "### Uploading exported chat file\n",
    "conversationPath = '/WhatsApp Chat with Psyan.txt' # chat file\n",
    "with open(conversationPath, encoding=\"utf-8\") as fp:\n",
    "    ### Skipping first line of the file because contains information related to something about end-to-end encryption\n",
    "    fp.readline() \n",
    "    messageBuffer = [] \n",
    "    date, time, author = None, None, None\n",
    "    while True:\n",
    "        line = fp.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        line = line.strip() \n",
    "        if startsWithDateAndTime(line): \n",
    "            if len(messageBuffer) > 0: \n",
    "                parsedData.append([date, time, author, ' '.join(messageBuffer)]) \n",
    "            messageBuffer.clear() \n",
    "            date, time, author, message = getDataPoint(line) \n",
    "            messageBuffer.append(message) \n",
    "        else:\n",
    "            messageBuffer.append(line)\n",
    "          \n",
    "df = pd.DataFrame(parsedData, columns=['Date', 'Time', 'Author', 'Message']) # Initialising a pandas Dataframe.\n",
    "### changing datatype of \"Date\" column.\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking shape of dataset.\n",
    "df.shape\n",
    "### Checking basic information of dataset\n",
    "df.info()\n",
    "### Checking no. of null values in dataset\n",
    "df.isnull().sum()\n",
    "### Checking head part of dataset\n",
    "df.head(50)\n",
    "### Checking tail part of dataset\n",
    "df.tail(50)\n",
    "### Droping Nan values from dataset\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape\n",
    "### Checking no. of authors of group\n",
    "df['Author'].nunique()\n",
    "### Checking authors of group\n",
    "df['Author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding one more column of \"Day\" for better analysis, here we use datetime library which help us to do this task easily.\n",
    "weeks = {\n",
    "0 : 'Monday',\n",
    "1 : 'Tuesday',\n",
    "2 : 'Wednesday',\n",
    "3 : 'Thrusday',\n",
    "4 : 'Friday',\n",
    "5 : 'Saturday',\n",
    "6 : 'Sunday'\n",
    "}\n",
    "df['Day'] = df['Date'].dt.weekday.map(weeks)\n",
    "### Rearranging the columns for better understanding\n",
    "df = df[['Date','Day','Time','Author','Message']]\n",
    "### Changing the datatype of column \"Day\".\n",
    "df['Day'] = df['Day'].astype('category')\n",
    "### Looking newborn dataset.\n",
    "df.head()\n",
    "### Counting number of letters in each message\n",
    "df['Letters'] = df['Message'].apply(lambda s : len(s))\n",
    "### Counting number of word's in each message\n",
    "df['Words'] = df['Message'].apply(lambda s : len(s.split(' ')))\n",
    "### Function to count number of links in dataset, it will add extra column and store information in it.\n",
    "\n",
    "URLPATTERN = r'(https?://S+)'\n",
    "df['Url_Count'] = df.Message.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n",
    "links = np.sum(df.Url_Count)\n",
    "### Function to count number of media in chat.\n",
    "MEDIAPATTERN = r'<Media omitted>'\n",
    "df['Media_Count'] = df.Message.apply(lambda x : re.findall(MEDIAPATTERN, x)).str.len()\n",
    "media = np.sum(df.Media_Count)\n",
    "### Looking updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_messages = df.shape[0]\n",
    "media_messages = df[df['Message'] == '<Media omitted>'].shape[0]\n",
    "links = np.sum(df.Url_Count)\n",
    "print('Group Chatting Stats : ')\n",
    "print('Total Number of Messages : {}'.format(total_messages))\n",
    "print('Total Number of Media Messages : {}'.format(media_messages))\n",
    "print('Total Number of Links : {}'.format(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.Author.unique()\n",
    "for i in range(len(l)):\n",
    "  ### Filtering out messages of particular user\n",
    "  req_df = df[df[\"Author\"] == l[i]]\n",
    "  ### Renaming the column name to avoid SyntaxError\n",
    "  req_df = req_df.rename(columns={'Word\\'s': 'Words'})\n",
    "\n",
    "  \n",
    "  ### req_df will contain messages of only one particular user\n",
    "  print(f'--> Stats of {l[i]} <-- ')\n",
    "  ### shape will print number of rows which indirectly means the number of messages\n",
    "  print('Total Message Sent : ', req_df.shape[0])\n",
    "  ### Word_Count contains of total words in one message. Sum of all words/ Total Messages will yield words per message\n",
    "  words_per_message = (np.sum(req_df['Words']))/req_df.shape[0]\n",
    "  w_p_m = (\"%.3f\" % round(words_per_message, 2))  \n",
    "  print('Average Words per Message : ', w_p_m)\n",
    "  ### media conists of media messages\n",
    "  media = sum(req_df[\"Media_Count\"])\n",
    "  print('Total Media Message Sent : ', media)\n",
    "  ### links consist of total links\n",
    "  links = sum(req_df[\"Url_Count\"])   \n",
    "  print('Total Links Sent : ', links)   \n",
    "  print()\n",
    "  print('----------------------------------------------------------n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Cloud of mostly used word in our Group\n",
    "text = \" \".join(review for review in df.Message)\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").generate(text)\n",
    "  ### Display the generated image:\n",
    "plt.figure( figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creates a list of unique Authors\n",
    "l = df.Author.unique()\n",
    "for i in range(len(l)):\n",
    "  ### Filtering out messages of particular user\n",
    "  req_df = df[df[\"Author\"] == l[i]]\n",
    "  ### req_df will contain messages of only one particular user\n",
    "  print(l[i],'  ->  ',req_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.Day.unique()\n",
    "for i in range(len(l)):\n",
    "  ### Filtering out messages of particular user\n",
    "  req_df = df[df[\"Day\"] == l[i]]\n",
    "  ### req_df will contain messages of only one particular user\n",
    "  print(l[i],'  ->  ',req_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "### Mostly Active Author in the Group\n",
    "plt.figure(figsize=(9,6))\n",
    "mostly_active = df['Author'].value_counts()\n",
    "### Top 10 peoples that are mostly active in our Group is : \n",
    "m_a = mostly_active.head(10)\n",
    "bars = m_a.index\n",
    "x_pos = np.arange(len(bars))\n",
    "m_a.plot.bar()\n",
    "plt.xlabel('Authors',fontdict={'fontsize': 14,'fontweight': 10})\n",
    "plt.ylabel('No. of messages',fontdict={'fontsize': 14,'fontweight': 10})\n",
    "plt.title('Mostly active member of Group',fontdict={'fontsize': 20,'fontweight': 8})\n",
    "plt.xticks(x_pos, bars)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = df[['Author','Words']].groupby('Author').sum()\n",
    "m_w = max_words.sort_values('Words',ascending=False).head(10)\n",
    "bars = m_a.index\n",
    "x_pos = np.arange(len(bars))\n",
    "m_w.plot.bar(rot=90)\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('No. of words')\n",
    "plt.title('Analysis of members who has used max. no. of words in his/her messages')\n",
    "plt.xticks(x_pos, bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Author'\n",
    "grouped_df = df.groupby(['Author'])\n",
    "\n",
    "# Use the sum() method to get the total number of words sent by each user\n",
    "total_words = grouped_df['Words'].sum()\n",
    "\n",
    "# Print the result\n",
    "print(total_words)\n",
    "for author, words in total_words.items():\n",
    "    print(f'{author} sent {words} words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analyser using Library \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['sentiment'] = df['Message'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_authors = df.groupby(\"Author\")[\"Message\"].apply(lambda x: x.tolist()).to_dict()\n",
    "for author in df_authors:\n",
    "    messages = df_authors[author]\n",
    "    compound_sentiment = [sia.polarity_scores(x)[\"compound\"] for x in messages]\n",
    "    df_authors[author] = compound_sentiment\n",
    "    negativity_scores = {}\n",
    "for author in df_authors:\n",
    "    negativity_scores[author] = sum([x<0 for x in df_authors[author]])\n",
    "\n",
    "plt.bar(negativity_scores.keys(), negativity_scores.values())\n",
    "plt.xlabel('Authors')\n",
    "plt.ylabel('Negativity scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentences = df[df['sentiment'] < 0]['Message'].tolist()\n",
    "positive_sentences = df[df['sentiment'] > 0]['Message'].tolist()\n",
    "\n",
    "print(\"Negative sentences:\")\n",
    "for sentence in negative_sentences:\n",
    "    print(sentence)\n",
    "    \n",
    "print(\"Positive sentences:\")\n",
    "for sentence in positive_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column that represents the number of days since the first message\n",
    "df[\"Day\"] = (df[\"Date\"] - df[\"Date\"].min()).dt.days\n",
    "\n",
    "# group the dataframe by the new column and calculate the mean negativity score for each day\n",
    "df_grouped = df.groupby(\"Day\").mean()\n",
    "\n",
    "#The module calculates a sentiment score between -1 and 1, where -1 represents a negative sentiment, 0 represents a neutral sentiment, \n",
    "# and 1 represents a positive sentiment. A negative score implies a negative sentiment in the text.\n",
    "# create a line chart\n",
    "plt.plot(df_grouped.index, df_grouped[\"sentiment\"])\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Negativity score')\n",
    "plt.title('Total negativity of the group over time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT EXECUTE\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json   \n",
    "\n",
    "def get_toxicity_score(text):\n",
    "    text = text.encode('utf-8')\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain',\n",
    "        'Ocp-Apim-Subscription-Key': '70fd210ab7c24175a34188db6153b416',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        'autocorrect': 'True',\n",
    "        'PII': 'False',\n",
    "        'classify': 'True',\n",
    "        'language': 'eng',\n",
    "    })\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('eastasia.api.cognitive.microsoft.com')\n",
    "        conn.request(\"POST\", \"/contentmoderator/moderate/v1.0/ProcessText/Screen?%s\" % params, text, headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        \n",
    "        print(data)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "     print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "df_authors = df.groupby(\"Author\")[\"Message\"].apply(lambda x: x.tolist()).to_dict()\n",
    "\n",
    "toxicity_scores = {}\n",
    "for author in df_authors:\n",
    " messages = df_authors[author][:500] # only checking the first 1000 messages\n",
    "for msg in messages:\n",
    " toxicity_score = get_toxicity_score(msg)\n",
    " if toxicity_score is not None:\n",
    "    if author not in toxicity_scores:\n",
    "        toxicity_scores[author] = []\n",
    "    toxicity_scores[author].append(toxicity_score)\n",
    "\n",
    "\n",
    "\n",
    "print(toxicity_scores)\n",
    "for author in toxicity_scores:\n",
    "    non_none_values = [val for val in toxicity_scores[author] if val is not None]\n",
    "    avg_toxicity = sum(non_none_values) / len(non_none_values)\n",
    "    plt.bar(author, avg_toxicity)\n",
    "\n",
    "plt.xlabel(\"Author\")\n",
    "plt.ylabel(\"Toxicity Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json   # GC-toxicity using Microsoft azure resource \n",
    "\n",
    "def get_toxicity_score(text):\n",
    "    text = text.encode('utf-8')\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain',\n",
    "        'Ocp-Apim-Subscription-Key': 'abc7a176fc97427fbf80863cd9c51eab',\n",
    "    }\n",
    "    params = urllib.parse.urlencode({\n",
    "        'autocorrect': 'True',\n",
    "        'PII': 'False',\n",
    "        'classify': 'True',\n",
    "        'language': 'eng',\n",
    "    })\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection('eastasia.api.cognitive.microsoft.com')\n",
    "        conn.request(\"POST\", \"/contentmoderator/moderate/v1.0/ProcessText/Screen?%s\" % params, text, headers)\n",
    "        response = conn.getresponse()\n",
    "        if response is not None and response.status == 200:\n",
    "            data = json.loads(response.read())\n",
    "            if data is not None and 'Classification' in data and 'ReviewRecommended' in data['Classification']:\n",
    "                toxicity_score = data[\"Classification\"][\"ReviewRecommended\"]\n",
    "                conn.close()\n",
    "                return toxicity_score\n",
    "    except Exception as e:\n",
    "        print(\"Error getting toxicity score:\", e)\n",
    "        return None\n",
    "    \n",
    "df_authors = df.groupby(\"Author\")[\"Message\"].apply(lambda x: x.tolist()).to_dict()\n",
    "\n",
    "toxicity_scores = {}\n",
    "for author in df_authors:\n",
    "    messages = df_authors[author][:50] # only checking the first 'n' messages. For full messages use messages = df_authors[author]\n",
    "    for msg in messages:\n",
    "        toxicity_score = get_toxicity_score(msg)\n",
    "        if toxicity_score is not None:\n",
    "            if author not in toxicity_scores:\n",
    "                toxicity_scores[author] = [toxicity_score]\n",
    "            else:\n",
    "                toxicity_scores[author].append(toxicity_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(toxicity_scores)\n",
    "authors = []\n",
    "avg_toxicity_scores = []\n",
    "for author in toxicity_scores:\n",
    "    non_none_values = [val for val in toxicity_scores[author] if val is not None]\n",
    "    avg_toxicity = sum(non_none_values) / len(non_none_values)\n",
    "    authors.append(author)\n",
    "    avg_toxicity_scores.append(avg_toxicity)\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(avg_toxicity_scores, labels=authors)\n",
    "plt.title(\"Author Toxicity Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Baye's algo to train and sentiment analysis model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "df['Sentiment'] = df['Message'].apply(get_sentiment)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Message'], df['Sentiment'], test_size=0.2, random_state=1)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_counts = cv.fit_transform(X_train)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_counts, y_train)\n",
    "\n",
    "X_test_counts = cv.transform(X_test)\n",
    "y_pred = nb.predict(X_test_counts)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nb.predict(X_test_counts) # predict the sentiment of test set\n",
    "\n",
    "# create two lists to store positive and negative sentences\n",
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "\n",
    "# loop through the test set and add sentences to corresponding list\n",
    "for i in range(len(X_test)):\n",
    "    if prediction[i] == 'positive':\n",
    "        positive_sentences.append(X_test.iloc[i])\n",
    "    elif prediction[i] == 'negative':\n",
    "        negative_sentences.append(X_test.iloc[i])\n",
    "\n",
    "# display positive sentences\n",
    "print(\"Positive Sentences:\")\n",
    "for sentence in positive_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# display negative sentences\n",
    "print(\"\\nNegative Sentences:\")\n",
    "for sentence in negative_sentences:\n",
    "    print(sentence)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
